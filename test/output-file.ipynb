{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.merge import add\n",
    "from keras.engine import InputSpec\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Deconvolution2D,  Conv2D,UpSampling2D,Cropping2D\n",
    "from VGG16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.applications.imagenet_utils import  preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "class InputNormalize(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InputNormalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        #x = (x - 127.5)/ 127.5\n",
    "        return x/255.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv_bn_relu(nb_filter, nb_row, nb_col,stride):   \n",
    "    def conv_func(x):\n",
    "        x = Conv2D(nb_filter, (nb_row, nb_col), strides=stride,padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        #x = LeakyReLU(0.2)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "    return conv_func\n",
    "\n",
    "\n",
    "\n",
    "#https://keunwoochoi.wordpress.com/2016/03/09/residual-networks-implementation-on-keras/\n",
    "def res_conv(nb_filter, nb_row, nb_col,stride=(1,1)):\n",
    "    def _res_func(x):\n",
    "        identity = Cropping2D(cropping=((2,2),(2,2)))(x)\n",
    "\n",
    "        a = Conv2D(nb_filter, (nb_row, nb_col), strides=stride, padding='valid')(x)\n",
    "        a = BatchNormalization()(a)\n",
    "        #a = LeakyReLU(0.2)(a)\n",
    "        a = Activation(\"relu\")(a)\n",
    "        a = Conv2D(nb_filter, (nb_row, nb_col), strides=stride, padding='valid')(a)\n",
    "        y = BatchNormalization()(a)\n",
    "\n",
    "        return  add([identity, y])\n",
    "\n",
    "    return _res_func\n",
    "\n",
    "\n",
    "def dconv_bn_nolinear(nb_filter, nb_row, nb_col,stride=(2,2),activation=\"relu\"):\n",
    "    def _dconv_bn(x):\n",
    "        #TODO: Deconvolution2D\n",
    "        #x = Deconvolution2D(nb_filter,nb_row, nb_col, output_shape=output_shape, subsample=stride, border_mode='same')(x)\n",
    "        #x = UpSampling2D(size=stride)(x)\n",
    "        x = UnPooling2D(size=stride)(x)\n",
    "        x = ReflectionPadding2D(padding=stride)(x)\n",
    "        x = Conv2D(nb_filter, (nb_row, nb_col), padding='valid')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activation)(x)\n",
    "        return x\n",
    "    return _dconv_bn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Denormalize(Layer):\n",
    "    '''\n",
    "    Custom layer to denormalize the final Convolution layer activations (tanh)\n",
    "    Since tanh scales the output to the range (-1, 1), we add 1 to bring it to the\n",
    "    range (0, 2). We then multiply it by 127.5 to scale the values to the range (0, 255)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Denormalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        '''\n",
    "        Scales the tanh output activations from previous layer (-1, 1) to the\n",
    "        range (0, 255)\n",
    "        '''\n",
    "\n",
    "        return (x + 1) * 127.5\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "class VGGNormalize(Layer):\n",
    "    '''\n",
    "    Custom layer to subtract the outputs of previous layer by 120,\n",
    "    to normalize the inputs to the VGG network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(VGGNormalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # No exact substitute for set_subtensor in tensorflow\n",
    "        # So we subtract an approximate value       \n",
    "        \n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, :, :, ::-1]       \n",
    "        x -= 120\n",
    "        #img_util.preprocess_image(style_image_path, img_width, img_height)\n",
    "        return x\n",
    "   \n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), dim_ordering='default', **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "\n",
    "        self.padding = padding\n",
    "        if isinstance(padding, dict):\n",
    "            if set(padding.keys()) <= {'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}:\n",
    "                self.top_pad = padding.get('top_pad', 0)\n",
    "                self.bottom_pad = padding.get('bottom_pad', 0)\n",
    "                self.left_pad = padding.get('left_pad', 0)\n",
    "                self.right_pad = padding.get('right_pad', 0)\n",
    "            else:\n",
    "                raise ValueError('Unexpected key found in `padding` dictionary. '\n",
    "                                 'Keys have to be in {\"top_pad\", \"bottom_pad\", '\n",
    "                                 '\"left_pad\", \"right_pad\"}.'\n",
    "                                 'Found: ' + str(padding.keys()))\n",
    "        else:\n",
    "            padding = tuple(padding)\n",
    "            if len(padding) == 2:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[0]\n",
    "                self.left_pad = padding[1]\n",
    "                self.right_pad = padding[1]\n",
    "            elif len(padding) == 4:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[1]\n",
    "                self.left_pad = padding[2]\n",
    "                self.right_pad = padding[3]\n",
    "            else:\n",
    "                raise TypeError('`padding` should be tuple of int '\n",
    "                                'of length 2 or 4, or dict. '\n",
    "                                'Found: ' + str(padding))\n",
    "\n",
    "        if dim_ordering not in {'tf'}:\n",
    "            raise ValueError('dim_ordering must be in {tf}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=4)] \n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        top_pad=self.top_pad\n",
    "        bottom_pad=self.bottom_pad\n",
    "        left_pad=self.left_pad\n",
    "        right_pad=self.right_pad        \n",
    "        \n",
    "        paddings = [[0,0],[left_pad,right_pad],[top_pad,bottom_pad],[0,0]]\n",
    "\n",
    "        \n",
    "        return tf.pad(x,paddings, mode='REFLECT', name=None)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        if self.dim_ordering == 'tf':\n",
    "            rows = input_shape[1] + self.top_pad + self.bottom_pad if input_shape[1] is not None else None\n",
    "            cols = input_shape[2] + self.left_pad + self.right_pad if input_shape[2] is not None else None\n",
    "\n",
    "            return (input_shape[0],\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    input_shape[3])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "            \n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding}\n",
    "        base_config = super(ReflectionPadding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))     \n",
    "    \n",
    "    \n",
    "class UnPooling2D(UpSampling2D):\n",
    "    def __init__(self, size=(2, 2)):\n",
    "        super(UnPooling2D, self).__init__(size)\n",
    "\n",
    "  \n",
    "    def call(self, x, mask=None):\n",
    "        shapes = x.get_shape().as_list() \n",
    "        w = self.size[0] * shapes[1]\n",
    "        h = self.size[1] * shapes[2]\n",
    "        return tf.image.resize_nearest_neighbor(x, (w,h))\n",
    "\n",
    "        \n",
    "\n",
    "class InstanceNormalize(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InstanceNormalize, self).__init__(**kwargs)\n",
    "        self.epsilon = 1e-3\n",
    "            \n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n",
    "        return tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, self.epsilon)))\n",
    "\n",
    "                                                 \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
